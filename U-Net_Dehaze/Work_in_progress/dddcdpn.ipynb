{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:21:59.606713Z","iopub.status.busy":"2024-05-01T17:21:59.605830Z","iopub.status.idle":"2024-05-01T17:22:00.627922Z","shell.execute_reply":"2024-05-01T17:22:00.626867Z","shell.execute_reply.started":"2024-05-01T17:21:59.606677Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'yolov5' already exists and is not an empty directory.\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T14:22:46.275908Z","iopub.status.busy":"2024-05-01T14:22:46.275120Z","iopub.status.idle":"2024-05-01T14:23:19.021155Z","shell.execute_reply":"2024-05-01T14:23:19.019636Z","shell.execute_reply.started":"2024-05-01T14:22:46.275862Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting yolov5\n","  Downloading yolov5-7.0.13-py37.py38.py39.py310-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: gitpython>=3.1.30 in /opt/conda/lib/python3.10/site-packages (from yolov5) (3.1.41)\n","Requirement already satisfied: matplotlib>=3.3 in /opt/conda/lib/python3.10/site-packages (from yolov5) (3.7.5)\n","Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from yolov5) (1.26.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (4.9.0.80)\n","Requirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from yolov5) (9.5.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from yolov5) (5.9.3)\n","Requirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (1.11.4)\n","Collecting thop>=0.1.1 (from yolov5)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.1.2)\n","Requirement already satisfied: torchvision>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (0.16.2)\n","Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (4.66.1)\n","Collecting ultralytics>=8.0.100 (from yolov5)\n","  Downloading ultralytics-8.2.6-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorboard>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.15.1)\n","Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from yolov5) (2.1.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (0.12.2)\n","Requirement already satisfied: setuptools>=65.5.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (69.0.3)\n","Collecting fire (from yolov5)\n","  Downloading fire-0.6.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: boto3>=1.19.1 in /opt/conda/lib/python3.10/site-packages (from yolov5) (1.26.100)\n","Collecting sahi>=0.11.10 (from yolov5)\n","  Downloading sahi-0.11.15-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: huggingface-hub>=0.12.0 in /opt/conda/lib/python3.10/site-packages (from yolov5) (0.22.2)\n","Collecting roboflow>=0.2.29 (from yolov5)\n","  Downloading roboflow-1.1.28-py3-none-any.whl.metadata (9.3 kB)\n","Collecting botocore<1.30.0,>=1.29.100 (from boto3>=1.19.1->yolov5)\n","  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.19.1->yolov5) (0.6.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython>=3.1.30->yolov5) (4.0.11)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (2024.2.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.12.0->yolov5) (4.9.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->yolov5) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->yolov5) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->yolov5) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->yolov5) (2024.2.2)\n","Collecting certifi>=2017.4.17 (from requests>=2.23.0->yolov5)\n","  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n","Collecting chardet==4.0.0 (from roboflow>=0.2.29->yolov5)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n","Collecting cycler>=0.10 (from matplotlib>=3.3->yolov5)\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl.metadata (722 bytes)\n","Collecting idna<4,>=2.5 (from requests>=2.23.0->yolov5)\n","  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n","Collecting opencv-python-headless==4.8.0.74 (from roboflow>=0.2.29->yolov5)\n","  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n","Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (1.16.0)\n","Requirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow>=0.2.29->yolov5) (0.10.1)\n","Collecting python-magic (from roboflow>=0.2.29->yolov5)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Collecting opencv-python>=4.1.1 (from yolov5)\n","  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: shapely>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from sahi>=0.11.10->yolov5) (1.8.5.post1)\n","Collecting pybboxes==0.1.6 (from sahi>=0.11.10->yolov5)\n","  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n","Collecting terminaltables (from sahi>=0.11.10->yolov5)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sahi>=0.11.10->yolov5) (8.1.7)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.51.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (2.26.1)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.5.2)\n","Requirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.20.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.4.1->yolov5) (3.0.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.0->yolov5) (3.1.2)\n","Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n","Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->yolov5) (2.4.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.0->yolov5) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->yolov5) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.4.1->yolov5) (3.2.2)\n","Downloading yolov5-7.0.13-py37.py38.py39.py310-none-any.whl (953 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.4/953.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading roboflow-1.1.28-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading sahi-0.11.15-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n","Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Downloading ultralytics-8.2.6-py3-none-any.whl (755 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.0/755.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=0e44ca1b4c154e35f8e3ec6d2b304ea77101efbce7a6308f707c566638ffb318\n","  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n","Successfully built fire\n","Installing collected packages: terminaltables, python-magic, pybboxes, opencv-python-headless, opencv-python, idna, fire, cycler, chardet, certifi, botocore, thop, sahi, ultralytics, roboflow, yolov5\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.9.0.80\n","    Uninstalling opencv-python-headless-4.9.0.80:\n","      Successfully uninstalled opencv-python-headless-4.9.0.80\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.9.0.80\n","    Uninstalling opencv-python-4.9.0.80:\n","      Successfully uninstalled opencv-python-4.9.0.80\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.6\n","    Uninstalling idna-3.6:\n","      Successfully uninstalled idna-3.6\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2024.2.2\n","    Uninstalling certifi-2024.2.2:\n","      Successfully uninstalled certifi-2024.2.2\n","  Attempting uninstall: botocore\n","    Found existing installation: botocore 1.34.69\n","    Uninstalling botocore-1.34.69:\n","      Successfully uninstalled botocore-1.34.69\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","aiobotocore 2.12.3 requires botocore<1.34.70,>=1.34.41, but you have botocore 1.29.165 which is incompatible.\n","albumentations 1.4.0 requires opencv-python>=4.9.0, but you have opencv-python 4.7.0.72 which is incompatible.\n","jupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\n","spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed botocore-1.29.165 certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 fire-0.6.0 idna-2.10 opencv-python-4.7.0.72 opencv-python-headless-4.8.0.74 pybboxes-0.1.6 python-magic-0.4.27 roboflow-1.1.28 sahi-0.11.15 terminaltables-3.1.10 thop-0.1.1.post2209072238 ultralytics-8.2.6 yolov5-7.0.13\n"]}],"source":["!pip install yolov5"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:22:09.262678Z","iopub.status.busy":"2024-05-01T17:22:09.261237Z","iopub.status.idle":"2024-05-01T17:22:09.271281Z","shell.execute_reply":"2024-05-01T17:22:09.270199Z","shell.execute_reply.started":"2024-05-01T17:22:09.262622Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['requirements.txt',\n"," '.dockerignore',\n"," 'detect.py',\n"," 'README.zh-CN.md',\n"," 'tutorial.ipynb',\n"," 'export.py',\n"," 'data',\n"," 'models',\n"," 'README.md',\n"," 'hubconf.py',\n"," 'CITATION.cff',\n"," 'segment',\n"," '.gitignore',\n"," '.github',\n"," 'pyproject.toml',\n"," '.gitattributes',\n"," 'benchmarks.py',\n"," 'classify',\n"," 'train.py',\n"," '.git',\n"," 'CONTRIBUTING.md',\n"," 'val.py',\n"," 'utils',\n"," 'LICENSE']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.listdir('/kaggle/working/yolov5')"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:22:12.913146Z","iopub.status.busy":"2024-05-01T17:22:12.912249Z","iopub.status.idle":"2024-05-01T17:22:12.917358Z","shell.execute_reply":"2024-05-01T17:22:12.916389Z","shell.execute_reply.started":"2024-05-01T17:22:12.913112Z"},"trusted":true},"outputs":[],"source":["from yolov5 import YOLOv5"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T20:51:21.766880Z","iopub.status.busy":"2024-05-01T20:51:21.765716Z","iopub.status.idle":"2024-05-01T20:51:22.383728Z","shell.execute_reply":"2024-05-01T20:51:22.382484Z","shell.execute_reply.started":"2024-05-01T20:51:21.766834Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Crop 1: Class ID=2, Class Name=car, Confidence=0.83, BBox=(1093, 422, 1189, 503)\n","Crop 2: Class ID=2, Class Name=car, Confidence=0.81, BBox=(2, 524, 215, 852)\n","Crop 3: Class ID=2, Class Name=car, Confidence=0.79, BBox=(833, 422, 964, 529)\n","Crop 4: Class ID=2, Class Name=car, Confidence=0.78, BBox=(723, 437, 819, 508)\n","Crop 5: Class ID=2, Class Name=car, Confidence=0.75, BBox=(948, 425, 1023, 490)\n","Crop 6: Class ID=5, Class Name=bus, Confidence=0.73, BBox=(0, 316, 240, 529)\n","Crop 7: Class ID=2, Class Name=car, Confidence=0.64, BBox=(295, 438, 448, 501)\n","Crop 8: Class ID=2, Class Name=car, Confidence=0.57, BBox=(1949, 344, 2047, 501)\n","Crop 9: Class ID=2, Class Name=car, Confidence=0.42, BBox=(0, 462, 119, 561)\n","Crop 10: Class ID=9, Class Name=traffic light, Confidence=0.40, BBox=(1348, 343, 1374, 385)\n","Crop 11: Class ID=2, Class Name=car, Confidence=0.39, BBox=(798, 405, 868, 468)\n","Crop 12: Class ID=2, Class Name=car, Confidence=0.37, BBox=(1763, 422, 1845, 472)\n","Crop 13: Class ID=56, Class Name=chair, Confidence=0.34, BBox=(1577, 462, 1655, 545)\n","Crop 14: Class ID=0, Class Name=person, Confidence=0.32, BBox=(1327, 419, 1347, 460)\n","Crop 15: Class ID=56, Class Name=chair, Confidence=0.30, BBox=(1640, 465, 1701, 548)\n","Crop 16: Class ID=56, Class Name=chair, Confidence=0.25, BBox=(1684, 469, 1760, 558)\n"]}],"source":["#tiny-yolov5s\n","\n","import numpy as np\n","from PIL import Image\n","from yolov5 import YOLOv5\n","import cv2\n","\n","model = YOLOv5('yolov5s.pt')  # Make sure the model file path is correct.\n","\n","def detect_and_crop(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    results = model.predict(np.array(image))\n","    detections = results.pandas().xyxy[0]  # Extract bounding boxes as DataFrame\n","\n","    crops = []\n","    details = []\n","    image_np = np.array(image)  # Convert PIL image to numpy array\n","\n","    for _, row in detections.iterrows():\n","        x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n","        conf = row['confidence']  # Extract confidence score\n","        class_id = row['class']  # Extract class ID\n","        class_name = row['name']  # Extract class name\n","\n","        crop = image_np[y1:y2, x1:x2]\n","        crops.append(crop)\n","        details.append((class_id, class_name, conf, (x1, y1, x2, y2)))  # Append all details as a tuple\n","\n","    return crops, details, image_np\n","\n","image_path = '/kaggle/input/idai710/foggy_images/test/berlin_000000_000019_leftImg8bit.png'  # Ensure the image path is correct.\n","\n","\n","crops, details, preprocessed_image = detect_and_crop(image_path)\n","\n","for idx, (crop, detail) in enumerate(zip(crops, details)):\n","    class_id, class_name, conf, bbox = detail\n","    print(f\"Crop {idx+1}: Class ID={class_id}, Class Name={class_name}, Confidence={conf:.2f}, BBox={bbox}\")\n","    save_path = f'crop_{idx + 1}_{class_name}_{int(conf * 100)}.png'  # Corrected file path format.\n","    cv2.imwrite(save_path, cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))  # Ensure conversion from RGB to BGR for OpenCV.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T17:26:28.236816Z","iopub.status.busy":"2024-05-01T17:26:28.236315Z","iopub.status.idle":"2024-05-01T20:11:57.007791Z","shell.execute_reply":"2024-05-01T20:11:57.006238Z","shell.execute_reply.started":"2024-05-01T17:26:28.236782Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1/10: 100%|██████████| 595/595 [14:01<00:00,  1.41s/it, train_loss=0.0506]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1/10, Train Loss: 0.0812\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 1/10: 100%|██████████| 100/100 [01:51<00:00,  1.12s/it, val_loss=0.0551]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Val Loss: 0.0620\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2/10: 100%|██████████| 595/595 [14:47<00:00,  1.49s/it, train_loss=0.0735]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2/10, Train Loss: 0.0616\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 2/10: 100%|██████████| 100/100 [02:00<00:00,  1.21s/it, val_loss=0.114]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Val Loss: 0.0932\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3/10: 100%|██████████| 595/595 [14:55<00:00,  1.50s/it, train_loss=0.0428]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3/10, Train Loss: 0.0522\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 3/10: 100%|██████████| 100/100 [02:04<00:00,  1.24s/it, val_loss=0.183]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Val Loss: 0.1637\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 4/10: 100%|██████████| 595/595 [14:58<00:00,  1.51s/it, train_loss=0.0869]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4/10, Train Loss: 0.0451\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 4/10: 100%|██████████| 100/100 [02:04<00:00,  1.25s/it, val_loss=0.0516]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Val Loss: 0.0639\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 5/10: 100%|██████████| 595/595 [15:04<00:00,  1.52s/it, train_loss=0.0415]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 5/10, Train Loss: 0.0423\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 5/10: 100%|██████████| 100/100 [01:58<00:00,  1.19s/it, val_loss=0.0485]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Val Loss: 0.0505\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 6/10: 100%|██████████| 595/595 [14:44<00:00,  1.49s/it, train_loss=0.0248]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 6/10, Train Loss: 0.0384\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 6/10: 100%|██████████| 100/100 [02:04<00:00,  1.24s/it, val_loss=0.145]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Val Loss: 0.1527\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 7/10: 100%|██████████| 595/595 [14:26<00:00,  1.46s/it, train_loss=0.0251]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 7/10, Train Loss: 0.0386\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 7/10: 100%|██████████| 100/100 [01:50<00:00,  1.10s/it, val_loss=0.0456]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Val Loss: 0.0553\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 8/10: 100%|██████████| 595/595 [13:39<00:00,  1.38s/it, train_loss=0.048] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 8/10, Train Loss: 0.0337\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 8/10: 100%|██████████| 100/100 [01:51<00:00,  1.11s/it, val_loss=0.0655]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Val Loss: 0.0685\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 9/10: 100%|██████████| 595/595 [13:48<00:00,  1.39s/it, train_loss=0.0191]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 9/10, Train Loss: 0.0328\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 9/10: 100%|██████████| 100/100 [02:00<00:00,  1.20s/it, val_loss=0.337]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Val Loss: 0.3411\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 10/10: 100%|██████████| 595/595 [15:00<00:00,  1.51s/it, train_loss=0.0671]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10/10, Train Loss: 0.0292\n"]},{"name":"stderr","output_type":"stream","text":["Validation Epoch 10/10: 100%|██████████| 100/100 [02:04<00:00,  1.25s/it, val_loss=0.0607]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Val Loss: 0.0645\n"]}],"source":["#DCDPN:\n","#new_version: \n","\n","import os\n","from PIL import Image\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","from tqdm import tqdm  \n","import os\n","\n","\n","\n","\n","class DehazingDataset(Dataset):\n","    def __init__(self, root_dir, subset='train', transform=None):\n","        self.root_dir = root_dir\n","        self.subset = subset\n","        self.transform = transform\n","        self.foggy_path = os.path.join(root_dir, 'foggy_images', subset)\n","        self.original_path = os.path.join(root_dir, 'original_images', subset)\n","        \n","        # Updated to filter for .png files\n","        self.image_files = [f for f in os.listdir(self.foggy_path) if f.endswith('.png') and os.path.isfile(os.path.join(self.foggy_path, f))]\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        file_name = self.image_files[idx]\n","        foggy_image_path = os.path.join(self.foggy_path, file_name)\n","        original_image_path = os.path.join(self.original_path, file_name)\n","\n","        foggy_image = Image.open(foggy_image_path).convert('RGB')\n","        original_image = Image.open(original_image_path).convert('RGB')\n","\n","        if self.transform:\n","            foggy_image = self.transform(foggy_image)\n","            original_image = self.transform(original_image)\n","\n","        return {'foggy': foggy_image, 'original': original_image}\n","\n","\n","class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","\n","        self.inc = DoubleConv(n_channels, 64)\n","        self.down1 = DoubleConv(64, 128)\n","        self.down2 = DoubleConv(128, 256)\n","        self.down3 = DoubleConv(256, 512)\n","        self.down4 = DoubleConv(512, 512)\n","        self.up1 = DoubleConv(1024, 256)\n","        self.up2 = DoubleConv(512, 128)\n","        self.up3 = DoubleConv(256, 64)\n","        self.up4 = DoubleConv(128, 64)\n","        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n","\n","        self.pool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.pool(x1)\n","        x2 = self.down1(x2)\n","        x3 = self.pool(x2)\n","        x3 = self.down2(x3)\n","        x4 = self.pool(x3)\n","        x4 = self.down3(x4)\n","        x5 = self.pool(x4)\n","        x5 = self.down4(x5)\n","\n","        x = self.upsample(x5)\n","        x = torch.cat([x, x4], dim=1)\n","        x = self.up1(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, x3], dim=1)\n","        x = self.up2(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, x2], dim=1)\n","        x = self.up3(x)\n","        x = self.upsample(x)\n","        x = torch.cat([x, x1], dim=1)\n","        x = self.up4(x)\n","        x = self.outc(x)\n","        return x\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# Assuming DehazingDataset and UNet are defined/imported as above or from another module\n","\n","root_dir = '/kaggle/input/idai710'\n","checkpoint_path = 'checkpoint.pth'\n","\n","transform = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","train_dataset = DehazingDataset(root_dir=root_dir, subset='train', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n","\n","val_dataset = DehazingDataset(root_dir=root_dir, subset='val', transform=transform)\n","val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = UNet(n_channels=3, n_classes=3).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Function to load the checkpoint\n","def load_checkpoint(filepath):\n","    if os.path.isfile(filepath):\n","        checkpoint = torch.load(filepath)\n","        model.load_state_dict(checkpoint['model_state'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state'])\n","        start_epoch = checkpoint['epoch'] + 1\n","        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n","        return start_epoch\n","    else:\n","        return 1\n","\n","# Function to save the checkpoint\n","def save_checkpoint(state, filename='checkpoint.pth'):\n","    torch.save(state, filename)\n","\n","num_epochs = 10\n","start_epoch = load_checkpoint(checkpoint_path)\n","\n","for epoch in range(start_epoch, num_epochs + 1):\n","    model.train()\n","    running_loss = 0.0\n","    train_progress = tqdm(train_loader, desc=f'Training Epoch {epoch}/{num_epochs}', total=len(train_loader))\n","    for data in train_progress:\n","        inputs, labels = data['foggy'].to(device), data['original'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        train_progress.set_postfix({'train_loss': loss.item()})\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    print(f'Epoch: {epoch}/{num_epochs}, Train Loss: {epoch_loss:.4f}')\n","\n","    # Validation phase\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_progress = tqdm(val_loader, desc=f'Validation Epoch {epoch}/{num_epochs}', total=len(val_loader))\n","    with torch.no_grad():\n","        for data in val_progress:\n","            inputs, labels = data['foggy'].to(device), data['original'].to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_running_loss += loss.item() * inputs.size(0)\n","            val_progress.set_postfix({'val_loss': loss.item()})\n","\n","    val_loss = val_running_loss / len(val_loader.dataset)\n","    print(f'Epoch {epoch}/{num_epochs}, Val Loss: {val_loss:.4f}')\n","    \n","    torch.save(model.state_dict(), 'dcdpn.pth')\n","\n","\n","    # Save checkpoint after every epoch\n","    save_checkpoint({\n","        'epoch': epoch,\n","        'model_state': model.state_dict(),\n","        'optimizer_state': optimizer.state_dict()\n","    }, checkpoint_path)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-01T20:51:40.836968Z","iopub.status.busy":"2024-05-01T20:51:40.835986Z","iopub.status.idle":"2024-05-01T20:51:41.947804Z","shell.execute_reply":"2024-05-01T20:51:41.946435Z","shell.execute_reply.started":"2024-05-01T20:51:40.836923Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/3698935993.py:20: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  image = image.resize((new_width, new_height), Image.ANTIALIAS)\n","/tmp/ipykernel_34/3698935993.py:20: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  image = image.resize((new_width, new_height), Image.ANTIALIAS)\n"]},{"name":"stdout","output_type":"stream","text":["Results saved: detected_results/berlin_000000_000019_leftImg8bit.png and detected_results/berlin_000000_000019_leftImg8bit_detections.txt\n"]}],"source":["from PIL import Image, ImageDraw, ImageFont\n","import numpy as np\n","import cv2\n","import os\n","\n","# Import the YOLOv5 model from its module\n","from yolov5 import YOLOv5\n","\n","# Initialize the YOLOv5 model with the specified weights\n","model = YOLOv5('yolov5x.pt')\n","\n","def load_image(image_path):\n","    \"\"\"Load an image, resize it, and pad it to a square of 640x640, maintaining aspect ratio.\"\"\"\n","    image = Image.open(image_path).convert(\"RGB\")\n","    original_width, original_height = image.size\n","    max_size = 640\n","    scale = min(max_size / original_width, max_size / original_height)\n","    new_width = int(original_width * scale)\n","    new_height = int(original_height * scale)\n","    image = image.resize((new_width, new_height), Image.ANTIALIAS)\n","    new_image = Image.new('RGB', (max_size, max_size))\n","    new_image.paste(image, ((max_size - new_width) // 2, (max_size - new_height) // 2))\n","    return np.array(new_image)\n","\n","def detect_objects(image_path):\n","    \"\"\"Use the YOLO model to detect objects in an image.\"\"\"\n","    image = load_image(image_path)\n","    results = model.predict(image)\n","    return results\n","\n","def draw_boxes(image_np, detections):\n","    \"\"\"Draw bounding boxes and labels on the image.\"\"\"\n","    image = Image.fromarray(image_np)\n","    draw = ImageDraw.Draw(image)\n","    font = ImageFont.load_default()\n","    for index, row in detections.iterrows():\n","        draw.rectangle([(row['xmin'], row['ymin']), (row['xmax'], row['ymax'])], outline=\"red\", width=2)\n","        label = f\"{row['name']} {row['confidence']:.2f}\"\n","        draw.text((row['xmin'], row['ymin']), label, fill=\"red\", font=font)\n","    return np.array(image)\n","\n","def save_detections(image_path, results, output_folder):\n","    \"\"\"Save the detection results as images with bounding boxes and as text files.\"\"\"\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","    image_np = load_image(image_path)\n","    detections = results.pandas().xyxy[0]\n","    image_with_boxes = draw_boxes(image_np, detections)\n","    output_image_path = os.path.join(output_folder, os.path.basename(image_path))\n","    Image.fromarray(image_with_boxes).save(output_image_path)\n","    output_text_path = os.path.join(output_folder, os.path.splitext(os.path.basename(image_path))[0] + '_detections.txt')\n","    with open(output_text_path, 'w') as f:\n","        for index, row in detections.iterrows():\n","            f.write(f\"Class ID: {row['class']}, Class Name: {row['name']}, Confidence: {row['confidence']:.2f}, BBox: [{row['xmin']}, {row['ymin']}, {row['xmax']}, {row['ymax']}]\\n\")\n","    print(f\"Results saved: {output_image_path} and {output_text_path}\")\n","\n","# Usage example\n","image_path = '/kaggle/input/idai710/foggy_images/test/berlin_000000_000019_leftImg8bit.png'\n","output_folder = 'detected_results'\n","results = detect_objects(image_path)\n","save_detections(image_path, results, output_folder)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4857154,"sourceId":8199363,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
