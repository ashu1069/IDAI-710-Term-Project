{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image, ImageStat\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DehazingDataset(Dataset):\n",
    "    def __init__(self, foggy_dir, bb_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            foggy_dir (string): Path to the directory with the foggy images.\n",
    "            bb_dir (string): Path to the directory with bounding box annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.foggy_dir = os.path.join(foggy_dir, 'JPEGImages')\n",
    "        self.bb_dir = os.path.join(bb_dir, 'Annotations')\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(self.foggy_dir) if os.path.isfile(os.path.join(self.foggy_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        foggy_image_path = os.path.join(self.foggy_dir, img_name)\n",
    "        bb_file_path = os.path.join(self.bb_dir, img_name.replace('.png', '.xml').replace('.jpg', '.xml'))\n",
    "\n",
    "        foggy_image = Image.open(foggy_image_path).convert('RGB')\n",
    "\n",
    "        # Assuming bounding box annotations are in XML and need parsing\n",
    "        bboxes = self.parse_bboxes(bb_file_path)\n",
    "\n",
    "        sample = {'foggy_image': foggy_image, 'bboxes': bboxes}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['foggy_image'] = self.transform(sample['foggy_image'])\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def parse_bboxes(self, xml_path):\n",
    "        # Implement XML parsing here\n",
    "        import xml.etree.ElementTree as ET\n",
    "        bboxes = []\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            bboxes.append({\n",
    "                'class_label': member.find('name').text,\n",
    "                'x_center': float(member.find('bndbox/xmin').text),\n",
    "                'y_center': float(member.find('bndbox/ymin').text),\n",
    "                'width': float(member.find('bndbox/xmax').text) - float(member.find('bndbox/xmin').text),\n",
    "                'height': float(member.find('bndbox/ymax').text) - float(member.find('bndbox/ymin').text)\n",
    "            })\n",
    "        return bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    # Extract images and bounding boxes from the batch\n",
    "    foggy_images = [item['foggy_image'] for item in batch]\n",
    "    bboxes = [item['bboxes'] for item in batch]\n",
    "\n",
    "    # Collate the images into a single tensor\n",
    "    foggy_images = torch.stack(foggy_images, dim=0)\n",
    "\n",
    "    # Bounding boxes are returned as is, since their sizes can vary and they cannot be stacked into a single tensor\n",
    "    return {'foggy_image': foggy_images, 'bboxes': bboxes}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory paths\n",
    "foggy_dir = '/home/stu12/s11/ak1825/idai710/Project/RTTS'\n",
    "bb_dir = '/home/stu12/s11/ak1825/idai710/Project/RTTS'\n",
    "\n",
    "# Define transformations, if any\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),  # Resize images to 640x640\n",
    "    transforms.ToTensor()         # Convert images to PyTorch tensors\n",
    "      # Normalize images\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = DehazingDataset(foggy_dir=foggy_dir, bb_dir=bb_dir, transform=transform)\n",
    "\n",
    "# Initialize the DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4322"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DehazeNetAttention(\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (e_conv1): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (e_conv2): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e_conv3): Conv2d(6, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (e_conv4): Conv2d(6, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (e_conv5): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (attention): SpatialAttentionLayer(\n",
       "    (conv1): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Adjust torch.load to map model to the correct device\n",
    "model = torch.load('/home/stu12/s11/ak1825/idai710/Project/Dehazed-Detection/AOD-NetX/aodnetX2.pt', map_location=device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_brightness(image):\n",
    "    stat = ImageStat.Stat(image)\n",
    "    r,g,b = stat.mean\n",
    "    brightness_value = np.sqrt(0.299 * (r**2) + 0.587 * (g**2) + 0.114 * (b**2))\n",
    "    return brightness_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_contrast(image):\n",
    "    image_gray = image.convert('L')\n",
    "    return np.std(image_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_color_shift(image):\n",
    "    \"\"\"Calculate the average distance of image colors from pure white.\"\"\"\n",
    "    # Convert the image into a NumPy array\n",
    "    pixels = np.array(image)\n",
    "    \n",
    "    # Define the RGB value for pure white\n",
    "    white = np.array([255, 255, 255])\n",
    "    \n",
    "    # Calculate the Euclidean distance from white for each pixel\n",
    "    distances = np.sqrt(np.sum((pixels - white) ** 2, axis=-1))\n",
    "    \n",
    "    # Calculate the average distance\n",
    "    avg_distance = np.mean(distances)\n",
    "    \n",
    "    # Normalize the average distance to a scale of 0 to 1 for consistency with other metrics\n",
    "    # Assuming the maximum possible distance from white is the distance from white to black (sqrt(3) * 255)\n",
    "    normalized_avg_distance = avg_distance / (np.sqrt(3) * 255)\n",
    "    \n",
    "    # Return the normalized average distance as the color shift metric\n",
    "    return normalized_avg_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_value, max_value):\n",
    "    \"\"\"Normalize the value to a [0, 1] scale.\"\"\"\n",
    "    return (value - min_value) / (max_value - min_value)\n",
    "\n",
    "def calculate_haze_index(image, min_brightness, max_brightness, min_contrast, max_contrast, min_color_shift, max_color_shift, weights={'brightness': 0.33, 'contrast': 0.33, 'color_shift': 0.34}):\n",
    "    # Calculate metrics\n",
    "    brightness = calculate_brightness(image)\n",
    "    contrast = calculate_contrast(image)\n",
    "    color_shift = calculate_color_shift(image)\n",
    "    \n",
    "    # Normalize metrics\n",
    "    norm_brightness = normalize(brightness, min_brightness, max_brightness)\n",
    "    norm_contrast = 1 - normalize(contrast, min_contrast, max_contrast) # Invert because higher contrast means less haze\n",
    "    norm_color_shift = normalize(color_shift, min_color_shift, max_color_shift)\n",
    "    \n",
    "    # Calculate weighted haze index\n",
    "    haze_index = (weights['brightness'] * norm_brightness +\n",
    "                  weights['contrast'] * norm_contrast +\n",
    "                  weights['color_shift'] * norm_color_shift)\n",
    "    return haze_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_brightness, max_brightness = 24, 233\n",
    "min_contrast, max_contrast = 8, 104\n",
    "min_color_shift, max_color_shift = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def psnr(target, prediction):\n",
    "#     mse = np.mean((target - prediction) ** 2)\n",
    "#     if mse == 0:\n",
    "#         return float('inf')\n",
    "#     max_pixel = 255.0\n",
    "#     return 20 * np.log10(max_pixel / np.sqrt(mse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_psnr = 0.0\n",
    "# num_images = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data in data_loader:\n",
    "#         foggy_images = data['foggy_image'].to(device)\n",
    "#         bounding_boxes = data['bboxes']  # Retrieve bounding boxes from data\n",
    "\n",
    "#         # Call the model's forward method with both foggy images and bounding boxes\n",
    "#         outputs = model(foggy_images, bounding_boxes)\n",
    "\n",
    "#         # Move tensors to CPU for PSNR calculation\n",
    "#         outputs = outputs.cpu().numpy()\n",
    "#         foggy_images = foggy_images.cpu().numpy()\n",
    "\n",
    "#         # Calculate PSNR for each image in the batch\n",
    "#         for i in range(foggy_images.shape[0]):\n",
    "#             # Convert tensors to proper scale if they were normalized (0-1 to 0-255)\n",
    "#             original_image_scaled = (foggy_images[i] * 255).astype(np.uint8)\n",
    "#             predicted_image_scaled = (outputs[i] * 255).astype(np.uint8)\n",
    "\n",
    "#             current_psnr = psnr(original_image_scaled, predicted_image_scaled)\n",
    "#             total_psnr += current_psnr\n",
    "#             num_images += 1\n",
    "\n",
    "# average_psnr = total_psnr / num_images\n",
    "# print(f'Average PSNR: {average_psnr:.2f} dB')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Haze Index (Foggy): 0.53\n",
      "Average Haze Index (Predicted): 0.54\n"
     ]
    }
   ],
   "source": [
    "total_haze_index_foggy = 0.0\n",
    "total_haze_index_predicted = 0.0\n",
    "num_images = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in data_loader:\n",
    "        foggy_images = data['foggy_image'].to(device)\n",
    "        bounding_boxes = data['bboxes']  # Retrieve bounding boxes from data\n",
    "\n",
    "        # Call the model's forward method with both foggy images and bounding boxes\n",
    "        outputs = model(foggy_images, bounding_boxes)\n",
    "\n",
    "        # Move tensors to CPU for haze index calculation\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        foggy_images = foggy_images.cpu().numpy()\n",
    "\n",
    "        # Calculate haze index for each image in the batch\n",
    "        for i in range(foggy_images.shape[0]):\n",
    "            # Convert tensors to proper scale if they were normalized (0-1 to 0-255)\n",
    "            foggy_image = foggy_images[i].squeeze().transpose((1, 2, 0))\n",
    "            predicted_image = outputs[i].squeeze().transpose((1, 2, 0))\n",
    "\n",
    "            # Convert tensors to images\n",
    "            foggy_image = (255 * foggy_image).astype(np.uint8)\n",
    "            predicted_image = (255 * predicted_image).astype(np.uint8)\n",
    "\n",
    "            foggy_image = Image.fromarray(foggy_image)\n",
    "            predicted_image = Image.fromarray(predicted_image)\n",
    "\n",
    "            # Calculate haze index for foggy image\n",
    "            haze_index_foggy = calculate_haze_index(foggy_image, min_brightness, max_brightness, min_contrast, max_contrast, min_color_shift, max_color_shift)\n",
    "            total_haze_index_foggy += haze_index_foggy\n",
    "\n",
    "            # Calculate haze index for predicted image\n",
    "            haze_index_predicted = calculate_haze_index(predicted_image, min_brightness, max_brightness, min_contrast, max_contrast, min_color_shift, max_color_shift)\n",
    "            total_haze_index_predicted += haze_index_predicted\n",
    "\n",
    "            num_images += 1\n",
    "\n",
    "# Calculate average haze index for foggy and predicted images\n",
    "average_haze_index_foggy = total_haze_index_foggy / num_images\n",
    "average_haze_index_predicted = total_haze_index_predicted / num_images\n",
    "\n",
    "print(f'Average Haze Index (Foggy): {average_haze_index_foggy:.2f}')\n",
    "print(f'Average Haze Index (Predicted): {average_haze_index_predicted:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
